{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Pipeline with MongoDB Vector Store </br>\n",
    "Author: Sanjit Verma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pymongo import MongoClient\n",
    "import logging\n",
    "import os, pprint\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MONGODB_URI = os.getenv('MONGODB_URI')\n",
    "db_name = os.getenv('MONGODB_DATABASE')\n",
    "collection_name = os.getenv('MONGODB_VECTORS')\n",
    "vector_search_idx = os.getenv('MONGODB_VECTOR_INDEX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client[db_name]\n",
    "collection = db[collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is dry?\n",
      "Answer: In the context of food or cooking, \"dry\" typically refers to a lack of moisture or liquid in a dish. For example, if a piece of meat is overcooked, it may become dry and tough because the moisture has been cooked out of it. Similarly, a cake or bread that has been baked for too long may become dry and crumbly.\n",
      "\n",
      "In a broader sense, \"dry\" can also refer to a lack of humidity or moisture in the air, which can have various effects on the environment and people's health. For example, dry air can lead to dry skin, throat irritation, and respiratory issues for some individuals.\n",
      "\n",
      "In the context of alcohol, \"dry\" can refer to a wine or a cocktail that is not sweet, but rather has a more acidic or tannic taste. Dry wines, for example, have minimal residual sugar content, resulting in a more crisp and tart flavor profile.\n",
      "\n",
      "In a geographical context, a \"dry\" climate refers to an area that receives very little precipitation, leading to arid conditions and limited vegetation. Deserts are typically characterized by dry climates, with low levels of rainfall and high evaporation rates.\n",
      "\n",
      "If there is a specific context in which you are asking about \"dry,\" please provide more information so I can give a more precise answer.\n",
      "\n",
      "Source documents:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "vector_search = MongoDBAtlasVectorSearch( # retrieve documents from MongoDB collection\n",
    "   embedding=OpenAIEmbeddings(disallowed_special=()),\n",
    "   collection=collection,\n",
    "   index_name=vector_search_idx,\n",
    ")\n",
    "\n",
    "retriever = vector_search.as_retriever( #This method configures the vector_search instance to retrieve documents based on similarity.\n",
    "   search_type = \"similarity\",\n",
    "   search_kwargs = {\"k\": 10, \"score_threshold\": 0.75} # top 10 most similar documents, , only return documents with a similarity score of 0.75 or higher \n",
    ")\n",
    "\n",
    "#Define the template used to format the input for the language model and provide a consistent response\n",
    "template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer or if it is not provided in the context, just say that you don't know, don't try to make up an answer.\n",
    "If the answer is in the context, dont say mentioned in the context.\n",
    "Please provide a detailed explanation and if applicable, give examples or historical context.\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "def format_docs(docs):\n",
    "   return \"\\n\\n\".join(doc.page_content for doc in docs) # This function formats the documents retrieved from MongoDB into a single string with each document separated by two newlines. \n",
    "\n",
    "rag_chain = (\n",
    "   # retriever first gets all relevant documents, then that is passed to the next step in the chain which is formatting docs (denoted by |)\n",
    "   { \"context\": retriever | format_docs, \"question\": RunnablePassthrough()} #runnable passthrough is used to pass the question to the next step in the chain without mods\n",
    "   | custom_rag_prompt\n",
    "   | llm\n",
    "   | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"what is dry?\"\n",
    "answer = rag_chain.invoke(question) # convert question to embedding and he most relevant documents based on the query's embedding are fetched from the MongoDB collection \n",
    "\n",
    "\n",
    "print(\"Question: \" + question)\n",
    "print(\"Answer: \" + answer)\n",
    "\n",
    "documents = retriever.get_relevant_documents(question)\n",
    "print(\"\\nSource documents:\")\n",
    "pprint.pprint(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "STEPS THAT OCCUR IN THE BACKGROUND when .invoke() is called on the rag_chain instance \n",
    "\n",
    "Step 1 INVOCATION: Invocation of rag chain with the question. Method call triggers the chains operation starting with the retriever\n",
    "\n",
    "Step 2 RETRIEVER FUNC: The retriever retrieves the most relevant documents from the MongoDB collection based on the question. Converts question into embedding and \n",
    "uses this to perform a similarity search in the database, retrieving documents that are contextually similar to the query.\n",
    "\n",
    "Step 3 DOC FORMATTING: The retrieved documents are passed to the next step in the chain, which is the format_docs function. This function formats the documents \n",
    "into a single string with each document separated by double newlines (basically processing output from retriever)\n",
    "\n",
    "Step 4 CONTEXT ASSEMBLY: The formatted documents and the question are passed to the custom_rag_prompt function to from complete context. RunnablePassthrough \n",
    "is highly important here to make sure no mods occur to question. \n",
    "\n",
    "Step 5 PROMPT TEMPLATE: template recieves step 4 and fills it out where context is replaced by the output from format_docs and question is replaced by the output from RunnablePasst\n",
    "\n",
    "Step 6 LANGUAGE MODEL: templated string are passed to the language model, which generates a response based on the input.model generates a response based on the input prompt, \n",
    "considering the provided context and directly addressing the query.\n",
    "\n",
    "Step 7 POST PARSE: Takes output and converts it into a string. If it is already a string, return it otherwise if it is a structured object, convert it to a string.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
