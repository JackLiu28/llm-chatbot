{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Pipeline Test with MongoDB Vector Store </br>\n",
    "Author: Sanjit Verma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pymongo import MongoClient\n",
    "import logging\n",
    "import os, pprint\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from functools import lru_cache\n",
    "\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MONGODB_URI = os.getenv('MONGODB_URI')\n",
    "db_name = os.getenv('MONGODB_DATABASE')\n",
    "collection_name = os.getenv('MONGODB_TEMPUSER')\n",
    "vector_search_idx = os.getenv('MONGODB_VECTOR_INDEX_TEMPUSER')\n",
    "openai.api_key = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client[db_name]\n",
    "collection = db[collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search = MongoDBAtlasVectorSearch( # retrieve documents from MongoDB collection\n",
    "   embedding=OpenAIEmbeddings(disallowed_special=()),\n",
    "   collection=collection,\n",
    "   index_name=vector_search_idx,\n",
    ")\n",
    "\n",
    "retriever = vector_search.as_retriever( #This method configures the vector_search instance to retrieve documents based on similarity.\n",
    "   search_type = \"similarity\",\n",
    "   search_kwargs = {\"k\": 5, \"score_threshold\": 0.75} # top 10 most similar documents, , only return documents with a similarity score of 0.75 or higher \n",
    ")\n",
    "\n",
    "#Define the template used to format the input for the language model and provide a consistent response\n",
    "template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer or if it is not provided in the context, just say that you don't know, don't try to make up an answer.\n",
    "If the answer is in the context, dont say mentioned in the context.\n",
    "Please provide a detailed explanation and if applicable, give examples or historical context.\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "def format_docs(docs):\n",
    "   return \"\\n\\n\".join(doc.page_content for doc in docs) # This function formats the documents retrieved from MongoDB into a single string with each document separated by two newlines. \n",
    "\n",
    "rag_chain = (\n",
    "   # retriever first gets all relevant documents, then that is passed to the next step in the chain which is formatting docs (denoted by |)\n",
    "   { \"context\": retriever | format_docs, \"question\": RunnablePassthrough()} #runnable passthrough is used to pass the question to the next step in the chain without mods\n",
    "   | custom_rag_prompt\n",
    "   | llm\n",
    "   | StrOutputParser()\n",
    ")\n",
    "\n",
    "# demonstrating caching of mongoDB queries + questions, very basic example actual \n",
    "MAX_CACHE_SIZE = 100\n",
    "@lru_cache(maxsize=MAX_CACHE_SIZE)\n",
    "def cached_query(question):\n",
    "    response = rag_chain.invoke(question)\n",
    "    return response\n",
    "\n",
    "RED = '\\033[91m'\n",
    "GREEN = '\\033[92m'\n",
    "YELLOW = '\\033[93m'\n",
    "RESET = '\\033[0m'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mCache Info: CacheInfo(hits=1, misses=3, maxsize=100, currsize=3)\u001b[0m\n",
      "\u001b[91mQuestion: How were the notes used?\u001b[0m\n",
      "\u001b[92mAnswer: The context provided does not specify what type of notes are being referred to, so it is difficult to provide a specific answer. \n",
      "\n",
      "Notes could refer to musical notes, in which case they are used in music to represent the pitch and duration of a sound. Musicians read and interpret notes on a musical staff to play or sing a piece of music. \n",
      "\n",
      "Notes could also refer to written messages or reminders. In this case, notes are used as a form of communication or to keep track of information. People use notes to jot down ideas, important points, to-do lists, or messages for themselves or others.\n",
      "\n",
      "Without more specific information on the type of notes being discussed, it is not possible to provide a more detailed answer.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source documents:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "question = \"How were the notes used?\"\n",
    "answer = cached_query(question) # convert question to embedding and he most relevant documents based on the query's embedding are fetched from the MongoDB collection \n",
    "\n",
    "\n",
    "print(f\"{YELLOW}Cache Info: {cached_query.cache_info()}{RESET}\")\n",
    "print(f\"{RED}Question: {question}{RESET}\")\n",
    "print(f\"{GREEN}Answer: {answer}{RESET}\")\n",
    "\n",
    "documents = retriever.get_relevant_documents(question)\n",
    "print(\"\\nSource documents:\")\n",
    "pprint.pprint(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "STEPS THAT OCCUR IN THE BACKGROUND when .invoke() is called on the rag_chain instance \n",
    "\n",
    "Step 1 INVOCATION: Invocation of rag chain with the question. Method call triggers the chains operation starting with the retriever\n",
    "\n",
    "Step 2 RETRIEVER FUNC: The retriever retrieves the most relevant documents from the MongoDB collection based on the question. Converts question into embedding and \n",
    "uses this to perform a similarity search in the database, retrieving documents that are contextually similar to the query.\n",
    "\n",
    "Step 3 DOC FORMATTING: The retrieved documents are passed to the next step in the chain, which is the format_docs function. This function formats the documents \n",
    "into a single string with each document separated by double newlines (basically processing output from retriever)\n",
    "\n",
    "Step 4 CONTEXT ASSEMBLY: The formatted documents and the question are passed to the custom_rag_prompt function to from complete context. RunnablePassthrough \n",
    "is highly important here to make sure no mods occur to question. \n",
    "\n",
    "Step 5 PROMPT TEMPLATE: template recieves step 4 and fills it out where context is replaced by the output from format_docs and question is replaced by the output from RunnablePasst\n",
    "\n",
    "Step 6 LANGUAGE MODEL: templated string are passed to the language model, which generates a response based on the input.model generates a response based on the input prompt, \n",
    "considering the provided context and directly addressing the query.\n",
    "\n",
    "Step 7 POST PARSE: Takes output and converts it into a string. If it is already a string, return it otherwise if it is a structured object, convert it to a string.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
