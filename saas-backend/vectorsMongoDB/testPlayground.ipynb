{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Pipeline Test with MongoDB Vector Store </br>\n",
    "Author: Sanjit Verma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pymongo import MongoClient\n",
    "import logging\n",
    "import os, pprint\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MONGODB_URI = os.getenv('MONGODB_URI')\n",
    "db_name = os.getenv('MONGODB_DATABASE')\n",
    "collection_name = os.getenv('MONGODB_VECTORS')\n",
    "vector_search_idx = os.getenv('MONGODB_VECTOR_INDEX')\n",
    "openai.api_key = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client[db_name]\n",
    "collection = db[collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mQuestion: Who are the authors of the textbook?\u001b[0m\n",
      "\u001b[92mAnswer: The authors of the textbook are Armando Fox and David Patterson.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "vector_search = MongoDBAtlasVectorSearch( # retrieve documents from MongoDB collection\n",
    "   embedding=OpenAIEmbeddings(disallowed_special=()),\n",
    "   collection=collection,\n",
    "   index_name=vector_search_idx,\n",
    ")\n",
    "\n",
    "retriever = vector_search.as_retriever( #This method configures the vector_search instance to retrieve documents based on similarity.\n",
    "   search_type = \"similarity\",\n",
    "   search_kwargs = {\"k\": 10, \"score_threshold\": 0.75} # top 10 most similar documents, , only return documents with a similarity score of 0.75 or higher \n",
    ")\n",
    "\n",
    "#Define the template used to format the input for the language model and provide a consistent response\n",
    "template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer or if it is not provided in the context, just say that you don't know, don't try to make up an answer.\n",
    "If the answer is in the context, dont say mentioned in the context.\n",
    "Please provide a detailed explanation and if applicable, give examples or historical context.\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "def format_docs(docs):\n",
    "   return \"\\n\\n\".join(doc.page_content for doc in docs) # This function formats the documents retrieved from MongoDB into a single string with each document separated by two newlines. \n",
    "\n",
    "rag_chain = (\n",
    "   # retriever first gets all relevant documents, then that is passed to the next step in the chain which is formatting docs (denoted by |)\n",
    "   { \"context\": retriever | format_docs, \"question\": RunnablePassthrough()} #runnable passthrough is used to pass the question to the next step in the chain without mods\n",
    "   | custom_rag_prompt\n",
    "   | llm\n",
    "   | StrOutputParser()\n",
    ")\n",
    "\n",
    "RED = '\\033[91m'\n",
    "GREEN = '\\033[92m'\n",
    "RESET = '\\033[0m'\n",
    "\n",
    "question = \"Who are the authors of the textbook?\"\n",
    "answer = rag_chain.invoke(question) # convert question to embedding and he most relevant documents based on the query's embedding are fetched from the MongoDB collection \n",
    "\n",
    "\n",
    "print(f\"{RED}Question: {question}{RESET}\")\n",
    "print(f\"{GREEN}Answer: {answer}{RESET}\")\n",
    "\n",
    "\n",
    "# documents = retriever.get_relevant_documents(question)\n",
    "# print(\"\\nSource documents:\")\n",
    "# pprint.pprint(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "STEPS THAT OCCUR IN THE BACKGROUND when .invoke() is called on the rag_chain instance \n",
    "\n",
    "Step 1 INVOCATION: Invocation of rag chain with the question. Method call triggers the chains operation starting with the retriever\n",
    "\n",
    "Step 2 RETRIEVER FUNC: The retriever retrieves the most relevant documents from the MongoDB collection based on the question. Converts question into embedding and \n",
    "uses this to perform a similarity search in the database, retrieving documents that are contextually similar to the query.\n",
    "\n",
    "Step 3 DOC FORMATTING: The retrieved documents are passed to the next step in the chain, which is the format_docs function. This function formats the documents \n",
    "into a single string with each document separated by double newlines (basically processing output from retriever)\n",
    "\n",
    "Step 4 CONTEXT ASSEMBLY: The formatted documents and the question are passed to the custom_rag_prompt function to from complete context. RunnablePassthrough \n",
    "is highly important here to make sure no mods occur to question. \n",
    "\n",
    "Step 5 PROMPT TEMPLATE: template recieves step 4 and fills it out where context is replaced by the output from format_docs and question is replaced by the output from RunnablePasst\n",
    "\n",
    "Step 6 LANGUAGE MODEL: templated string are passed to the language model, which generates a response based on the input.model generates a response based on the input prompt, \n",
    "considering the provided context and directly addressing the query.\n",
    "\n",
    "Step 7 POST PARSE: Takes output and converts it into a string. If it is already a string, return it otherwise if it is a structured object, convert it to a string.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
